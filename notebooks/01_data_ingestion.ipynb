{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Ingestion Notebook\n",
        "This notebook demonstrates how to ingest data from various sources into Azure Data Lake Storage Gen2.\n",
        "\n",
        "## Prerequisites\n",
        "- Azure Synapse workspace configured\n",
        "- Data Lake Storage Gen2 connected\n",
        "- Appropriate permissions set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from datetime import datetime\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"DataIngestion\").getOrCreate()\n",
        "print(f\"Spark version: {spark.version}\")\n",
        "print(f\"Ingestion started at: {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration - Update these values\n",
        "storage_account_name = \"your_storage_account_name\"\n",
        "container_name = \"synapsefs\"\n",
        "raw_data_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/raw/\"\n",
        "bronze_data_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/bronze/\"\n",
        "\n",
        "print(f\"Raw data path: {raw_data_path}\")\n",
        "print(f\"Bronze data path: {bronze_data_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ingest CSV Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define schema for customer data\n",
        "customer_schema = StructType([\n",
        "    StructField(\"CustomerID\", StringType(), False),\n",
        "    StructField(\"CustomerName\", StringType(), False),\n",
        "    StructField(\"CustomerType\", StringType(), True),\n",
        "    StructField(\"Email\", StringType(), True),\n",
        "    StructField(\"Phone\", StringType(), True),\n",
        "    StructField(\"City\", StringType(), True),\n",
        "    StructField(\"State\", StringType(), True),\n",
        "    StructField(\"Country\", StringType(), True),\n",
        "    StructField(\"Region\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Read CSV file\n",
        "customer_df = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"false\") \\\n",
        "    .schema(customer_schema) \\\n",
        "    .csv(f\"{raw_data_path}customers.csv\")\n",
        "\n",
        "# Add audit columns\n",
        "customer_df = customer_df \\\n",
        "    .withColumn(\"ingestion_date\", current_timestamp()) \\\n",
        "    .withColumn(\"source_file\", input_file_name())\n",
        "\n",
        "print(f\"Loaded {customer_df.count()} customer records\")\n",
        "customer_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ingest JSON Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read JSON files\n",
        "product_df = spark.read \\\n",
        "    .option(\"multiline\", \"true\") \\\n",
        "    .json(f\"{raw_data_path}products.json\")\n",
        "\n",
        "# Add audit columns\n",
        "product_df = product_df \\\n",
        "    .withColumn(\"ingestion_date\", current_timestamp()) \\\n",
        "    .withColumn(\"source_file\", input_file_name())\n",
        "\n",
        "print(f\"Loaded {product_df.count()} product records\")\n",
        "product_df.printSchema()\n",
        "product_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ingest Parquet Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read Parquet files\n",
        "sales_df = spark.read.parquet(f\"{raw_data_path}sales/*.parquet\")\n",
        "\n",
        "# Add audit columns\n",
        "sales_df = sales_df \\\n",
        "    .withColumn(\"ingestion_date\", current_timestamp()) \\\n",
        "    .withColumn(\"source_file\", input_file_name())\n",
        "\n",
        "print(f\"Loaded {sales_df.count()} sales records\")\n",
        "sales_df.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Quality Checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for null values in key columns\n",
        "print(\"Customer Data Quality:\")\n",
        "customer_df.select([count(when(col(c).isNull(), c)).alias(c) for c in customer_df.columns]).show()\n",
        "\n",
        "print(\"\\nProduct Data Quality:\")\n",
        "product_df.select([count(when(col(c).isNull(), c)).alias(c) for c in product_df.columns]).show()\n",
        "\n",
        "print(\"\\nSales Data Quality:\")\n",
        "sales_df.select([count(when(col(c).isNull(), c)).alias(c) for c in sales_df.columns]).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicates\n",
        "customer_duplicates = customer_df.count() - customer_df.dropDuplicates([\"CustomerID\"]).count()\n",
        "product_duplicates = product_df.count() - product_df.dropDuplicates([\"ProductID\"]).count()\n",
        "\n",
        "print(f\"Customer duplicates: {customer_duplicates}\")\n",
        "print(f\"Product duplicates: {product_duplicates}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Write to Bronze Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write customer data to Bronze layer\n",
        "customer_df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"ingestion_date\") \\\n",
        "    .parquet(f\"{bronze_data_path}customers/\")\n",
        "\n",
        "print(\"Customer data written to Bronze layer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write product data to Bronze layer\n",
        "product_df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"ingestion_date\") \\\n",
        "    .parquet(f\"{bronze_data_path}products/\")\n",
        "\n",
        "print(\"Product data written to Bronze layer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write sales data to Bronze layer with partitioning\n",
        "sales_df.write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"ingestion_date\") \\\n",
        "    .parquet(f\"{bronze_data_path}sales/\")\n",
        "\n",
        "print(\"Sales data written to Bronze layer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print ingestion summary\n",
        "print(\"=\" * 50)\n",
        "print(\"Data Ingestion Summary\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Ingestion completed at: {datetime.now()}\")\n",
        "print(f\"Customer records: {customer_df.count()}\")\n",
        "print(f\"Product records: {product_df.count()}\")\n",
        "print(f\"Sales records: {sales_df.count()}\")\n",
        "print(\"=\" * 50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
