{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Data to SQL Pool\n",
        "This notebook loads transformed data from Silver layer to Dedicated SQL Pool.\n",
        "\n",
        "## Process\n",
        "1. Read from Silver layer\n",
        "2. Apply final transformations\n",
        "3. Load into dimension tables (SCD Type 2)\n",
        "4. Load into fact tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"LoadToSQLPool\").getOrCreate()\n",
        "print(f\"Load process started at: {datetime.now()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "storage_account_name = \"your_storage_account_name\"\n",
        "container_name = \"synapsefs\"\n",
        "silver_data_path = f\"abfss://{container_name}@{storage_account_name}.dfs.core.windows.net/silver/\"\n",
        "\n",
        "# SQL Pool connection details\n",
        "sql_pool_name = \"EnterpriseDW\"\n",
        "sql_server_name = \"your_synapse_workspace.sql.azuresynapse.net\"\n",
        "sql_username = \"sqladmin\"\n",
        "sql_password = \"your_password\"  # Use Key Vault in production\n",
        "\n",
        "# JDBC connection string\n",
        "jdbc_url = f\"jdbc:sqlserver://{sql_server_name}:1433;database={sql_pool_name};encrypt=true;trustServerCertificate=false;hostNameInCertificate=*.sql.azuresynapse.net;loginTimeout=30;\"\n",
        "\n",
        "connection_properties = {\n",
        "    \"user\": sql_username,\n",
        "    \"password\": sql_password,\n",
        "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
        "}\n",
        "\n",
        "print(f\"Silver data path: {silver_data_path}\")\n",
        "print(f\"SQL Pool: {sql_pool_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Read Silver Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read Silver layer data\n",
        "customer_silver = spark.read.format(\"delta\").load(f\"{silver_data_path}customers/\")\n",
        "product_silver = spark.read.format(\"delta\").load(f\"{silver_data_path}products/\")\n",
        "sales_silver = spark.read.format(\"delta\").load(f\"{silver_data_path}sales/\")\n",
        "\n",
        "print(\"Silver data loaded:\")\n",
        "print(f\"Customers: {customer_silver.count()}\")\n",
        "print(f\"Products: {product_silver.count()}\")\n",
        "print(f\"Sales: {sales_silver.count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Dimension Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare customer dimension for SCD Type 2\n",
        "dim_customer = customer_silver.select(\n",
        "    col(\"CustomerID\"),\n",
        "    col(\"CustomerName\"),\n",
        "    col(\"CustomerType\"),\n",
        "    col(\"Email\"),\n",
        "    col(\"Phone\"),\n",
        "    col(\"City\"),\n",
        "    col(\"State\"),\n",
        "    col(\"Country\"),\n",
        "    col(\"Region\"),\n",
        "    lit(None).cast(\"string\").alias(\"CustomerSegment\"),\n",
        "    current_date().alias(\"EffectiveDate\"),\n",
        "    lit(None).cast(\"date\").alias(\"EndDate\"),\n",
        "    lit(True).alias(\"IsCurrent\")\n",
        ")\n",
        "\n",
        "print(f\"Prepared {dim_customer.count()} customer dimension records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare product dimension for SCD Type 2\n",
        "dim_product = product_silver.select(\n",
        "    col(\"ProductID\"),\n",
        "    col(\"ProductName\"),\n",
        "    col(\"ProductDescription\"),\n",
        "    col(\"Category\"),\n",
        "    col(\"SubCategory\"),\n",
        "    col(\"Brand\"),\n",
        "    col(\"SKU\"),\n",
        "    col(\"UnitPrice\"),\n",
        "    col(\"StandardCost\"),\n",
        "    col(\"ListPrice\"),\n",
        "    current_date().alias(\"EffectiveDate\"),\n",
        "    lit(None).cast(\"date\").alias(\"EndDate\"),\n",
        "    lit(True).alias(\"IsCurrent\")\n",
        ")\n",
        "\n",
        "print(f\"Prepared {dim_product.count()} product dimension records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Fact Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare sales fact data\n",
        "fact_sales = sales_silver.select(\n",
        "    col(\"OrderID\"),\n",
        "    col(\"OrderLineNumber\"),\n",
        "    regexp_replace(date_format(col(\"OrderDate\"), \"yyyyMMdd\"), \"-\", \"\").cast(\"int\").alias(\"DateKey\"),\n",
        "    col(\"CustomerKey\"),\n",
        "    col(\"ProductKey\"),\n",
        "    col(\"StoreKey\"),\n",
        "    col(\"EmployeeKey\"),\n",
        "    col(\"PromotionKey\"),\n",
        "    col(\"Quantity\"),\n",
        "    col(\"UnitPrice\"),\n",
        "    col(\"UnitCost\"),\n",
        "    col(\"DiscountAmount\"),\n",
        "    col(\"TaxAmount\"),\n",
        "    col(\"net_revenue\").alias(\"SalesAmount\"),\n",
        "    col(\"total_cost\").alias(\"CostAmount\"),\n",
        "    col(\"gross_profit\").alias(\"GrossProfitAmount\"),\n",
        "    col(\"gross_profit\").alias(\"NetProfitAmount\"),\n",
        "    col(\"OrderDate\"),\n",
        "    col(\"ShipDate\"),\n",
        "    col(\"PaymentMethod\"),\n",
        "    col(\"ShippingMethod\")\n",
        ")\n",
        "\n",
        "print(f\"Prepared {fact_sales.count()} sales fact records\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load to SQL Pool Using Staging Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to write dataframe to SQL Pool\n",
        "def write_to_sql_pool(df, table_name, write_mode=\"append\"):\n",
        "    \"\"\"\n",
        "    Write DataFrame to SQL Pool\n",
        "    Args:\n",
        "        df: Spark DataFrame\n",
        "        table_name: Target table name (schema.table)\n",
        "        write_mode: append or overwrite\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(f\"Writing {df.count()} records to {table_name}...\")\n",
        "        \n",
        "        # Write to SQL Pool using JDBC\n",
        "        df.write \\\n",
        "            .mode(write_mode) \\\n",
        "            .jdbc(url=jdbc_url, table=table_name, properties=connection_properties)\n",
        "        \n",
        "        print(f\"Successfully wrote to {table_name}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error writing to {table_name}: {str(e)}\")\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Alternative: Use COPY INTO for better performance\n",
        "def load_using_copy_into(df, table_name, staging_path):\n",
        "    \"\"\"\n",
        "    Load data using COPY INTO for better performance\n",
        "    Args:\n",
        "        df: Spark DataFrame\n",
        "        table_name: Target table name\n",
        "        staging_path: Staging path in ADLS\n",
        "    \"\"\"\n",
        "    # Write to staging location as Parquet\n",
        "    df.write.mode(\"overwrite\").parquet(staging_path)\n",
        "    \n",
        "    # Execute COPY INTO command\n",
        "    copy_sql = f\"\"\"\n",
        "    COPY INTO {table_name}\n",
        "    FROM '{staging_path}'\n",
        "    WITH (\n",
        "        FILE_TYPE = 'PARQUET',\n",
        "        CREDENTIAL = (IDENTITY = 'Managed Identity')\n",
        "    )\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"Executing COPY INTO for {table_name}...\")\n",
        "    # Execute using JDBC or SQLAlchemy\n",
        "    # spark.sql(copy_sql)  # If using linked service"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dimension Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load customer dimension (staging table for SCD Type 2 processing)\n",
        "write_to_sql_pool(dim_customer, \"staging.CustomerStaging\", \"overwrite\")\n",
        "print(\"Customer dimension staged\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load product dimension (staging table for SCD Type 2 processing)\n",
        "write_to_sql_pool(dim_product, \"staging.ProductStaging\", \"overwrite\")\n",
        "print(\"Product dimension staged\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Fact Tables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load sales fact data\n",
        "write_to_sql_pool(fact_sales, \"fact.FactSales\", \"append\")\n",
        "print(\"Sales fact data loaded\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Execute SQL Procedures for SCD Type 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute stored procedures for SCD Type 2 updates\n",
        "# This would typically be done via a pipeline activity\n",
        "print(\"SCD Type 2 updates should be executed via SQL stored procedures:\")\n",
        "print(\"- dim.UpdateCustomerSCD\")\n",
        "print(\"- dim.UpdateProductSCD\")\n",
        "print(\"\\nExecute these procedures in the Synapse SQL Pool\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verify Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read back from SQL Pool to verify\n",
        "try:\n",
        "    sales_verify = spark.read \\\n",
        "        .jdbc(url=jdbc_url, table=\"fact.FactSales\", properties=connection_properties)\n",
        "    \n",
        "    print(f\"Verified {sales_verify.count()} records in fact.FactSales\")\n",
        "    print(\"\\nSample records:\")\n",
        "    sales_verify.show(5)\n",
        "except Exception as e:\n",
        "    print(f\"Error verifying load: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print load summary\n",
        "print(\"=\" * 70)\n",
        "print(\"Data Load Summary\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Load completed at: {datetime.now()}\")\n",
        "print(\"\\nRecords Loaded:\")\n",
        "print(f\"  Customers (staged): {dim_customer.count()}\")\n",
        "print(f\"  Products (staged): {dim_product.count()}\")\n",
        "print(f\"  Sales facts: {fact_sales.count()}\")\n",
        "print(\"\\nNext Steps:\")\n",
        "print(\"  1. Execute SCD Type 2 stored procedures\")\n",
        "print(\"  2. Update statistics\")\n",
        "print(\"  3. Refresh aggregated views\")\n",
        "print(\"  4. Validate data quality\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
